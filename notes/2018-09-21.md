# Exploration of NAB evaluations

NAB provides anomaly detection evaluations by providing datasets (both artificial and real-cause) with known anomalies.

Scores are determined when an anomaly is detected by the algorithm in relation to an anomaly window.

All window sizes are of equal length `(0.10 * L)/(|Anomalies|)`, where L = Length of the File, Anomalies = Set of anomalies.
NAB states that the `0.10` is just randomly chosen, stating that the differences between scores with different window sizes `0.5 -> 0.20` was not statistically significant.

Scoring within the window is done with a sigmoidal function `(A_tp - A_fp)*(1/(1 + e^(5y)))` where `y` is the position in the window and `y=0` is the end of the window, `A_tp` are true positives, `A_fp` are false positives. The earlier in the window the anomaly is detected, the higher the score. If more than on anomaly is detected in the window, only the earliest anomaly is used in the calculation. 

All of these methodologies seem sound enough to provide an semi-accurate picture of how well a given anomaly detection algorithm/system performs.


There seem to be numerous issues with the `realKnownCause` datasets and their flagged anomalies. From what I have found at least one of the following issues is in each of the datasets:

* Anomaly is flagged too late, consequently, good detection algorithms would find the anomaly too early and get deducted for a "false positive"
   * `machine_temperature_system_failure` anomaly entry for `2013-12-16 17:25:00.000000` is far into the anomaly period. The window should have started much earlier.
* Anomaly is incorrectly flagged or missing.
   * `ambient_temperature_system_failure` entry for `2013-08-06 20:00:00` is obviously anomalous, but not flagged
* Some anomalous behavior disregards future data
   * `machine_temperature_system_failure` anomaly entry for `2013-12-11 06:00:00.000000` is only anomalous if later data is not taken into account

It would be nice to know how each of these datasets were gathered, and the methodologies used. These `realKnownCause` datasets may be taking into account external domain knowledge that is not available for the detector. Additionally, they are input by humans who know when the anomaly became an issue for system efficacy, which is problematic as the anomalous window may be well before the `knownCause` became apparent to humans.

I would caution using the `realKnownCause` data in conjunction with NAB as the only measure for an anomaly detection algorithm's/system's effectiveness. 

# Done today

* HLRC for stop data feed https://github.com/elastic/elasticsearch/pull/33946
* HLRC for start data feed https://github.com/elastic/elasticsearch/pull/33898 
* Opened a PR to make more settings available for the enduser (and dynamic): https://github.com/elastic/elasticsearch/pull/33961
* Additional performance metrics gathering for machine learning jobs
* Learned more about our Graphs plugins (that stuff is incredible)
