#Done today:

* Finished the work for PR: https://github.com/elastic/elasticsearch/pull/34654 and double checked behavior through manual testing. All looks well and will probably merge into master and backport to 6.6 tomorrow
* More studying around the elastic search certification. Was pointed to some additional resources and will also do the labs on Eng I/II again
   * https://www.elastic.co/webinars/preparing-for-the-elastic-certified-engineer-exam
   * https://training.elastic.co/exam/elastic-certified-engineer#objectives
   * The webinar was especially helpful 
* Reviewed some PRs
* More investigation and thinking about how we are going to handle missing data due to latency. My though processes are as follows:


* Best Option
   * This should be done within the DatafeedJob or the Manager in a parallel future scheduled thread. 
      * + Algorithmically simple as total hits are compared for the same query executed at different wall clock times
      * - Do not know WHAT/WHY data was missed, only that data was late to be indexed and our query did not catch it
* Other options
   * Instead of simply gathering hits, we actually gather the missed document IDs. 
      * - This could be costly in memory as a Set of seen IDs would have to be continually kept and updated for given windows (possibly many windows).
      * - No benefit for the ML job side as discrete past documents being sent to the C++ side is not supported and likely will never be.
      * + A benefit could be that the enduser has some metadata around the document (like what Ingest point/pipeline, etc.) that allows them to remediate an ingesting delay issue
   * Have a continually running process that keeps track of the data feed jobâ€™s stats and the indexed document stats to track divergences
      * - Hard to gather at what document timestamp the divergences takes place
      * - Interjecting with the statistics from the data feed (what ranged query was done at what wall clock time)
      * - Complexity of another persistent task that really should be a task tied directly with a single data feed. 
      * + Do not have to fiddle too much with the DatafeedJob/Manager and could potentially just use net new code running in parallel (complexities with what query was ran when still arise)

* Some next steps will also have to be considered as we start to see how many missing data issues are actually due to Ingest time latency.  
